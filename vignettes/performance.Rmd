---
title: "Performance Testing"
output: bookdown::html_document2
vignette: >
  %\VignetteIndexEntry{performance}
  %\VignetteEngine{knitr::bookdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r echo=FALSE}
library(optmatch)
```
The goal of this document is provide some basic performance testing of the
**optmatch** package. In most matching workflows, a user first creates
distances, produces one or more matches, assesses balance on a given match,
and, provided balance meets necessary requirements, proceeds to analysis. Of
these tasks, **optmach** is responsible for distance creation and
creating the match. These two tasks will be considered separately. The
remainder of this document lays out the performance testing strategy and then
implements it for the distance creation and matching routines in
*optmatch*.

# Performance Testing Strategy

**R** provides built in execution profiling through the **Rprof**
function (and a similarly named command line option). When invoked, this
function regularly interrupts normal processing and writes out the current
call stack to a file. This information can be used to attribute the portion of
run time attributable different functions.

```{r echo=FALSE}
library(profr)
perfcex <- 0.5 # makes text more readable in plots
minlabel <- 0.05 # add labels for smaller segments of time
```

For interpretation of this data, we rely on the **profr** package, which
provides some graphical summaries to help make raw profile data more manageable.

Since we suspect that creation of large data sets may account for a sizeable
portion of the runtime of large problems, we also profile memory usage
with simulated data described in its own section.

# Simulated Data

```{r echo=FALSE}
load("setup.rda")
```

Before proceeding to the actual profiling, we begin by creating some simulated
data.^[See the file **setup.R** for the supporting code.]
We use `r N` individuals, with `r sum(DATA[, "Z"])` receiving the
treatment condition.
Treatment assignment is based on a
simple linear model of 3 covariates (two Normal variables and one categorical
variable with 5 levels).

For most of the problems, we create a model of treatment assignment and then
pull out the linear predictors from that model.
Figure \@ref(fig:logits-density))
shows the distributions of the predicted probabilities for the treated and
control groups (henceforth the *propensity score*).

```{r logits-density, echo=FALSE, fig.align="center", fig.width=5, fig.height=5, fig.cap="Relative distribution of predicted probabilities for simulated treated and control units."}
library(lattice)
densityplot(~ predicted, groups = DATA$Z, xlab = "Linear predictors")
```

# Distance Creation

We begin by benchmarking dense distance creation.^[See the file
**distance.R** for implementation details.] The **match_on.numeric**
method has the least complicated interface and applies the least
pre-processing to the input. Figure \@ref(fig:distance-dense) shows the profile data for
using **match_on.numeric** with the propensity score.

```{r distance-dense, echo=FALSE, fig.align="center", fig.width=7, fig.height=4, fig.cap="Profiling diagram for dense distance creation for `N =` `r N` units."}
load("distance.rda")
par(cex = perfcex) # this makes the text in the plots a more maneagable size.
plot(benchmark.dense, minlabel = minlabel)
```

There are two ways to create sparse problems. In the first, we use the
**caliper** argument (for the **numeric** and **glm** methods
of **match_on**). This argument looks over all the treated and control
values and computes which treated and control units should be compared, and
then computes the exact distances between them. Applying a caliper width of 1
to the simulated data, leads to a sparse matrix with
`r round(100 * length(result.sparse.caliper) / (N*N), 1)`% finite
entries. Figure \@ref{fig:sparse-caliper} shows the profiling data for this
process.

The alternative to the **caliper** argument is the **within**
argument.
This method is more general, applying to all **match_on** methods, but can sometimes
require generating a dense matrix first (though not always --- the
**exactMatch** function doesn't require a dense matrix). To use the
**within** argument, we use **exactMatch** create two subproblems
based on the categorical covariate.
`r round(100 * length(result.sparse.within) / (N*N), 1)`% finite
entries. Figure \@ref{fig:sparse-within} shows the profiling results when using
the **within** argument.

```{r sparse-caliper, echo=FALSE, fig.align="center", fig.cap="Profiling diagram for caliper based sparse distance creation for `N =` `r N` units."}
par(cex = perfcex)
plot(benchmark.sparse.caliper, minlabel = minlabel)
```

```{r sparse-within, echo=FALSE, fig.align="center", fig.cap="Profiling diagram for within based sparse distance creation for `N =` `r N` units."}
par(cex = perfcex)
plot(benchmark.sparse.within, minlabel = minlabel)
```

# **match_on** Methods

These additional methods add some pre-processing to the distance creation.
These methods can work on sparse problems, but to keep things simple, these examples
just create dense matrices. The **glm** method is a relatively small
wrapper around the **numeric** method used in the previous examples.
Figure \@ref{fig:distance-glm} shows the profiling data for **glm**
method, which we would expect to look very similar to
Figure \@ref{fig:distance-dense}, the dense matrix problem from the previous
section.

```{r distance-glm, echo=FALSE, fig.align="center", fig.cap="Profiling diagram for **glm** distance creation for `N =` `r N` units."}
par(cex = perfcex)
plot(benchmark.glm, minlabel = minlabel)
```

Figure \@ref{fig:distance-formula} shows the
profiling data for using the **formula** method. This method, by default,
creates a squared Mahalanobis distance between treated and control pairs (Euclidean
distances scaled by the variance-covariance matrix).
Figure \@ref{fig:distance-formula} shows the profiling data for using the
**formula** method. This plot is expected to be very different than
the previous as all the previous examples were based a simple absolute
difference of a 1-D vector. In this task, however, we have to compute a
variance-covariance matrix and then produce a series of multiplications to
compute the squared distances. There may be opportunities to improve both
components of the distance creation. Additionally, they may not scale in the
same way, with one dominating for small problems and the other for large. This
plot does not provide any information the scaling nature of the function.

```{r distance-formula, echo=FALSE, fig.align="center", fig.cap="Profiling diagram for formula (Mahalanobis) distance creation for `N =` `r N` units."}
par(cex = perfcex)
plot(benchmark.formula, minlabel = minlabel)
```
# Matching

```{r echo=FALSE}
load("matching.rda")
```

Now that distance creation code has been benchmarked, we now consider the
matching process itself. We reuse the earlier distance objects. To keep things
simple, most of these matches will be computed using **fullmatch** at
default settings. The process of matching, as currently implemented in
**optmatch** can be broken down into the following steps:

\begin{enumerate}
  \item The specification for the match is check for basic sanity.
  \item If there are clear subproblems (such as those created using
  **exactMatch**), the problem is broken into multiple parts, and each
  of the next steps is run for each component.
  \item From the matrix representation of the distances (which is potentially
  sparse), a new matrix is formed with a row for each valid comparison. The data in
  each row is the treated unit, the control unit, and the distance (an
  adjacency matrix).
  \item This representation is passed to the Fortran solver, along with the
  fixed elements of the matching process.
  \item The results of the solver and turned into an **optmatch** object,
  a factor where the names corresponds to the names of the units and the
  levels to the matched groups. This object also stores additional information
  about the match as attributes. (If the match was split into multiple
  components, it is combined here.)
\end{enumerate}

We should expect to see these phases in each of the next figures.
Figure \@ref{fig:match-dense} shows the matching process applied to the dense
distance matrix from the previous section. Figure \@ref{fig:match-sparse} shows
a profiling information for the **caliper** argument based sparse distance
matrix. Figure \@ref{fig:match-sparse-strat} shows the profiling data for the
stratified, sparse problem (which can be split up into separate calls to the
solver)

```{r match-dense, echo=FALSE, fig.align="center", fig.cap="Profiling diagram for dense matrix based matching for `N =` `r N` units."}
par(cex = perfcex)
plot(benchmark.matching.dense, minlabel = minlabel)
```

```{r match-sparse, echo=FALSE, fig.align="center", fig.cap="Profiling diagram for sparse matrix based matching for `N =` `r N` units."}
par(cex = perfcex)
plot(benchmark.matching.sparse, minlabel = minlabel)
```

```{r match-sparse-strat, echo=FALSE, fig.align="center", fig.cap="Profiling diagram for stratified sparse matrix based matching for `N =` `r N` units."}
par(cex = perfcex)
plot(benchmark.matching.sparse.strat, minlabel = minlabel)
```

The previous tests used **fullmatch** with the default arguments. To test
out the use of the various constraint arguments, we call **pairmatch**
on the dense distance problem. In addition to fixing the minimum and maximum
number of controls per matched set to 1, the **pairmatch** function inspects the
distance object to set appropriate values for **omit.fraction**, the
argument that allows a portion of the control group to be discarded.
Figure \@ref{fig:match-pairmatch} shows these profiling data.

```{r match-pairmatch, echo=FALSE, fig.align="center", fig.cap="Profiling diagram for dense matrix pairmatching for `N =` `r N` units."}
par(cex = perfcex)
plot(benchmark.matching.pairmatch, minlabel = minlabel)
```

# **mdist** and **match_on**

In version 0.7 of **optmatch** and earlier, the primary method of
creating distances was to use the **mdist** function. In version 0.8,
**match_on** was added as a more comprehensive tool, specifically one
that allowed for arbitrary sparseness of distances matrices.

```{r echo=FALSE}
load("mdist.rda")
```

```{r echo=FALSE, fig.align="center", fig.cap="{Profiling results for a stratified mdist distance problem."}
par(cex = perfcex)
plot(benchmark.mdist, minlabel = minlabel)
```

# Appendix {-}

## Scaling

```{r echo=FALSE}
load("scaling.rda")
```

This section considers how the two functions scale up as the problem size gets
bigger. We create problems sets of size `N = 2K` with `K` treatment and `K`
control units. Figure \@ref{fig:scaling} shows the run time of the two functions as `K` is
increased. The `y` axis is logged showing that both methods grow roughly
quadratically, as we would expect from the nature of the algorithms. Of
course, the absolute run time of **match_on** is orders of magnitude
worse.

```{r scaling, echo=FALSE, fig.align="center", fig.cap="Run times for increasingly large problems for the **match_on** (red) and **mdist** (blue) functions."}
plot(match_on ~ k, data = times, type = 'l', col = "red", log = "y")
lines(mdist ~ k, data = times, col = "blue", log = "y")
```

# Environment

```r
sessionInfo()
```
